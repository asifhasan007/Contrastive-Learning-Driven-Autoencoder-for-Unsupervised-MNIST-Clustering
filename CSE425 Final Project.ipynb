{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "parquet_path = r\"C:\\Users\\Asif\\Downloads\\train.parquet\"\n",
    "parquet_data = pq.read_table(parquet_path).to_pandas()\n",
    "\n",
    "print(f\"Parquet file columns: {parquet_data.columns}\")\n",
    "print(f\"Number of samples: {len(parquet_data)}\")\n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "sample_img = parquet_data['image'].iloc[0]\n",
    "\n",
    "for i, row in enumerate(parquet_data.itertuples()):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processing image {i}/{len(parquet_data)}\")\n",
    "\n",
    "    img_bytes = row.image['bytes']\n",
    "\n",
    "    img_pil = Image.open(io.BytesIO(img_bytes))\n",
    "\n",
    "    img_array = np.array(img_pil)\n",
    "\n",
    "    if len(img_array.shape) == 3 and img_array.shape[2] > 1:\n",
    "        img_array = np.mean(img_array, axis=2).astype(np.uint8)\n",
    "\n",
    "    images.append(img_array)\n",
    "    labels.append(row.label)\n",
    "\n",
    "\n",
    "image_data = np.stack(images)\n",
    "label_data = np.array(labels)\n",
    "\n",
    "print(f\"Successfully extracted {len(images)} images\")\n",
    "print(f\"Image data shape: {image_data.shape}\")\n",
    "print(f\"Label data shape: {label_data.shape}\")\n",
    "\n",
    "train_images = torch.tensor(image_data, dtype=torch.float32)\n",
    "train_labels = torch.tensor(label_data, dtype=torch.long)\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "\n",
    "train_images = train_images.unsqueeze(1)\n",
    "\n",
    "print(f\"Final tensor shape: {train_images.shape}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices = np.arange(len(train_images))\n",
    "train_idx, valid_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=label_data)\n",
    "\n",
    "valid_images = train_images[valid_idx]\n",
    "valid_labels = train_labels[valid_idx]\n",
    "train_images = train_images[train_idx]\n",
    "train_labels = train_labels[train_idx]\n",
    "\n",
    "print(f\"Data loaded successfully.\")\n",
    "print(f\"Train set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(valid_images)} images\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "valid_dataset = TensorDataset(valid_images, valid_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_images[i, 0].numpy(), cmap='gray')\n",
    "    plt.title(f\"Label: {train_labels[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('mnist_samples22.png')\n",
    "plt.show()\n",
    "\n",
    "class EnhancedAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=10):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, stride=2), \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.MaxPool2d(2, stride=2), \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 64 * 7 * 7),\n",
    "            nn.BatchNorm1d(64 * 7 * 7),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "\n",
    "def contrastive_loss(embeddings, labels, temperature=0.5):\n",
    "    batch_size = embeddings.size(0)\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    \n",
    "    mask = torch.eq(labels, labels.T).float()\n",
    "    mask = mask.fill_diagonal_(0)\n",
    "    \n",
    "    embeddings_normalized = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    logits = torch.matmul(embeddings_normalized, embeddings_normalized.T) / temperature\n",
    "    logits = logits - torch.max(logits, dim=1, keepdim=True)[0] \n",
    "\n",
    "    exp_logits = torch.exp(logits)\n",
    "    log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True))\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1).clamp(min=1e-8)\n",
    "    \n",
    "    return -mean_log_prob_pos.mean()\n",
    "\n",
    "def train_enhanced_model(model, train_loader, valid_loader, epochs=30, alpha=0.8, beta=0.2):\n",
    "    device = next(model.parameters()).device\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    recon_criterion = nn.MSELoss()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            recon, z = model(data)\n",
    "            \n",
    "            rec_loss = recon_criterion(recon, data)\n",
    "            cont_loss = contrastive_loss(z, labels)\n",
    "            loss = alpha * rec_loss + beta * cont_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in valid_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                recon, z = model(data)\n",
    "                \n",
    "                rec_loss = recon_criterion(recon, data)\n",
    "                cont_loss = contrastive_loss(z, labels)\n",
    "                loss = alpha * rec_loss + beta * cont_loss\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        valid_loss /= len(valid_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.6f}, \"\n",
    "              f\"Valid Loss: {valid_loss:.6f}\")\n",
    "        scheduler.step(valid_loss)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('enhanced_training_history2.png')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def extract_embeddings(dataloader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            _, z = model(data)\n",
    "            embeddings.append(z.cpu().numpy())\n",
    "            labels.append(target.numpy())\n",
    "    \n",
    "    return np.vstack(embeddings), np.concatenate(labels)\n",
    "\n",
    "def improved_clustering(embeddings, true_labels):\n",
    "   \n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    n_clusters_range = [8, 9, 10, 11, 12]\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for n_clusters in n_clusters_range:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=30, max_iter=500, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(scaled_embeddings)\n",
    "        score = silhouette_score(scaled_embeddings, cluster_labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"K-Means with {n_clusters} clusters - Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"\\nBest number of clusters: {best_n_clusters}\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters=best_n_clusters, n_init=50, max_iter=1000, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_embeddings)\n",
    "    \n",
    "    silhouette = silhouette_score(scaled_embeddings, cluster_labels)\n",
    "    davies_bouldin = davies_bouldin_score(scaled_embeddings, cluster_labels)\n",
    "    \n",
    "    print(f\"Final Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    \n",
    "    return cluster_labels, scaled_embeddings\n",
    "\n",
    "def main():\n",
    " \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = EnhancedAutoencoder(latent_dim=10).to(device)\n",
    "    \n",
    "    model = train_enhanced_model(model, train_loader, valid_loader, epochs=30)\n",
    "    \n",
    "    print(\"Extracting embeddings...\")\n",
    "    embeddings, true_labels = extract_embeddings(train_loader, model)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "    print(\"Performing improved clustering...\")\n",
    "    cluster_labels, scaled_embeddings = improved_clustering(embeddings, true_labels)\n",
    "\n",
    "    print(\"Generating t-SNE visualization...\")\n",
    "    subset_size = min(5000, len(embeddings))\n",
    "    subset_indices = np.random.choice(len(embeddings), subset_size, replace=False)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=40, n_iter=1000, learning_rate=200, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(scaled_embeddings[subset_indices])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    scatter1 = axes[0].scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], \n",
    "                              c=cluster_labels[subset_indices], cmap='tab10', alpha=0.7)\n",
    "    axes[0].set_title(\"K-Means Clustering\")\n",
    "    plt.colorbar(scatter1, ax=axes[0])\n",
    "\n",
    "    scatter2 = axes[1].scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], \n",
    "                              c=true_labels[subset_indices], cmap='tab10', alpha=0.7)\n",
    "    axes[1].set_title(\"True Labels\")\n",
    "    plt.colorbar(scatter2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_mnist_clustering2.png')\n",
    "    plt.show()\n",
    "    print(\"Visualization saved to 'enhanced_mnist_clustering2.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
